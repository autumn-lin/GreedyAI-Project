{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1： 模型理论与应用\n",
    "1. 在第一部分主要以证明和推导为主。以下几个问题都是比较经典的问题，会对模型的深入理解会有很大的帮助。 特别是对于逻辑回归的二次导数的求解过程可以用来证明一个函数是否凸函数。挑战一下吧！ \n",
    "\n",
    "2. 另外，试着也借助这个机会学习一下latex的使用，在后续写文档的时候后可以帮上很多忙， 双击每一个cell,就可以看到数学表达式。 小小建议：把简历也写成LATEX形式，显得更加专业。有一些线上的编辑器可以参考：https://www.sharelatex.com/templates/cv-or-resume"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1.1 逻辑回归相关 (20分)\n",
    "假设我们有训练数据$D=\\{(\\mathbf{x}_1,y_1),...,(\\mathbf{x}_n,y_n)\\}$, 其中$(\\mathbf{x}_i,y_i)$为每一个样本，而且$\\mathbf{x}_i$是样本的特征并且$\\mathbf{x}_i\\in \\mathcal{R}^D$, $y_i$代表样本数据的标签（label）, 取值为$0$或者$1$. 在逻辑回归中，模型的参数为$(\\mathbf{w},b)$。对于向量，我们一般用粗体来表达。请回答以下问题。最好用Markdown自带的Latex来编写。（如果实在不行，可以手写然后拍照完放入word或者转成PDF，作为独立的文件来提交）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) 在逻辑回归模型下，请写出目标函数（objective function）, 也就是我们需要\"最小化\"的目标（也称之为损失函数或者loss function)，不需要考虑正则 （3分）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$L(\\mathbf{w},b)=$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) 求出$L(\\mathbf{w},b)$的梯度（或者计算导数），需要必要的中间过程。（3分）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{\\partial L(\\mathbf{w},b)}{\\partial \\mathbf{w}}=$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{\\partial L(\\mathbf{w},b)}{\\partial b}=$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) 请写出基于梯度下降法（batch）的对于$\\mathbf{w}$和$b$的更新 （3分）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$w^{t+1}=$\n",
    "\n",
    "$b^{t+1}=$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(d) 假设在(a)的基础上加了一个L2正则项，请写出基于梯度下降法（batch）的对于$\\mathbf{w}$和$b$的更新 （3分）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$w^{t+1}=$\n",
    "\n",
    "$b^{t+1}=$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来我们来证明逻辑回归函数是凸函数。假设一个函数是凸函数，我们则可以得出局部最优解即为全局最优解，所以假设我们通过随机梯度下降法等手段找到最优解\n",
    "时我们就可以确认这个解就是全局最优解。证明凸函数的方法有很多种，在这里我们介绍一种方法，就是基于二次求导大于等于0。比如给定一个函数$f(x)=x^2-3x+3$，做两次\n",
    "求导之后即可以得出$f''(x)=2 > 0$，所以这个函数就是凸函数。类似的，这种理论也应用于多元变量中的函数上。在多元函数上，只要证明二阶导数是posititive semidefinite即可以。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(e) 在(b)的基础上接着对$\\mathbf{w}$求导（等于二阶导数，二阶导数的维度为$D\\times D$），这个二阶导数也称之为Hessian Matrix(https://en.wikipedia.org/wiki/Hessian_matrix) 对于矩阵、向量的求导请参考：https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf （4分）\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{\\partial^2 \\mathcal{L}}{\\partial^2 \\mathbf{w}}=$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(f) 请说明在(e)的得出来的Hessian Matrix是Positive Definite. 提示：为了证明一个$D\\times D$的矩阵$H$为Positive Semidefinite，需要证明对于任意一个非零向量$v\\in \\mathcal{R}^D$, 需要得出$v^{T}Hv >=0$ （4分）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "请推导或者说明："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2： 情感分析项目 (80分)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本项目的目标是基于用户提供的评论，通过算法自动去判断其评论是正面的还是负面的情感。比如给定一个用户的评论：\n",
    "- 评论1： “我特别喜欢这个电器，我已经用了3个月，一点问题都没有！”\n",
    "- 评论2： “我从这家淘宝店卖的东西不到一周就开始坏掉了，强烈建议不要买，真实浪费钱”\n",
    "\n",
    "对于这两个评论，第一个明显是正面的，第二个是负面的。 我们希望搭建一个AI算法能够自动帮我们识别出评论是正面还是负面。\n",
    "\n",
    "情感分析的应用场景非常丰富，也是NLP技术在不同场景中落地的典范。比如对于一个证券领域，作为股民，其实比较关注舆论的变化，这个时候如果能有一个AI算法自动给网络上的舆论做正负面判断，然后把所有相关的结论再整合，这样我们可以根据这些大众的舆论，辅助做买卖的决策。 另外，在电商领域评论无处不在，而且评论已经成为影响用户购买决策的非常重要的因素，所以如果AI系统能够自动分析其情感，则后续可以做很多有意思的应用。 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "情感分析是文本处理领域经典的问题。整个系统一般会包括几个模块：\n",
    "- 数据的抓取： 通过爬虫的技术去网络抓取相关文本数据\n",
    "- 数据的清洗/预处理：在本文中一般需要去掉无用的信息，比如各种标签（HTML标签），标点符号，停用词等等\n",
    "- 把文本信息转换成向量： 这也成为特征工程，文本本身是不能作为模型的输入，只有数字（比如向量）才能成为模型的输入。所以进入模型之前，任何的信号都需要转换成模型可识别的数字信号（数字，向量，矩阵，张量...)\n",
    "- 选择合适的模型以及合适的评估方法。 对于情感分析来说，这是二分类问题（或者三分类：正面，负面，中性），所以需要采用分类算法比如逻辑回归，朴素贝叶斯，神经网络，SVM等等。另外，我们需要选择合适的评估方法，比如对于一个应用，我们是关注准确率呢，还是关注召回率呢？ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在本次项目中，我们已经给定了训练数据和测试数据，它们分别是 train.positive.txt, train.negative.txt， test_combined.txt. 请注意训练数据和测试数据的格式不一样，详情请见文件内容。 整个项目你需要完成以下步骤：\n",
    "\n",
    "数据的读取以及清洗： 从给定的.txt中读取内容，并做一些数据清洗，这里需要做几个工作： （1） 文本的读取，需要把字符串内容读进来。 （2）去掉无用的字符比如标点符号，多余的空格，换行符等 （3） 分词\n",
    "把文本转换成TF-IDF向量： 这部分直接可以利用sklearn提供的TfidfVectorizer类来做。\n",
    "利用逻辑回归模型来做分类，并通过交叉验证选择最合适的超参数\n",
    "利用支持向量机做分类，并通过交叉验证选择神经网络的合适的参数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File Reading: 文本读取 （5分）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['手感超好，而且黑色相比白色在转得时候不容易眼花，找童年的记忆啦。', '！！！！！', '先付款的   有信用', '价格 质量 售后 都很满意', '书的质量和印刷都不错，字的大小也刚刚好，很清楚，喜欢', '超级值得看的一个电影', '今天突然看到卓越有卖这个的，可是韩国不是卖没有了吗。虽然是引进版的，可是之前也卖没有了。卓越从哪里找出来的啊', '江大，继续写', '还不错，得多跟着练才能跟的上~~', '我前天两本一起买还是42.8元的，怎么一下子就降价了。。。。 但书还是对我有很大帮助的。而且发货速度很快，隔天就到了~！', '内容自不必细说，80后的人都知道，质量比较好，虽然不是尽善尽美，但企事业算是不错了。', '不是的话，看了这本书后我相信那就是真有时间隧道。', '发货迅速，书的质量也很好~很满意~', '作为一本指导性质的编程类书籍，这本书将各种设计模式演绎的淋漓尽致，通俗易懂，看后使人获益匪浅！设计模式只是开始，要真正的会用、用好还是需要实际的磨练。', '绝对不容错过的收藏！', '出售和购买盗版都是违法行为，你们还敢随便在这里瞎起哄。自己想想自己像是什么人吧', '不知道拨号数字大不大，话筒音量大不大。。。。', '神秘园的曲子总是能打动每一个人心中最真的地方，太好听了~', '上次买五月天的专辑和几本书，我特别说明了要包装牢固，joyo用了一个箱子，我很感动。这次也是cd和几本书，也是有包装的特别说明，结果竟然只有胶带，气死我了！还好，没有破损，要不然对不起布兰妮啊！ joyo的服务怎么每况愈下呢？']\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "\n",
    "def read_data(path, is_pos=None):\n",
    "    \"\"\"\n",
    "    给定文件的路径， 读取文件\n",
    "    path: path to the data\n",
    "    is_pos: 是否数据是positive samples\n",
    "    return: (list of review texts, list of labels)\n",
    "    \"\"\"\n",
    "    reviews, labels = [], []\n",
    "    with open(path, 'r', encoding='utf-8') as file:\n",
    "        review_start = False\n",
    "        # 放文本的\n",
    "        review_text = []\n",
    "        for line in file:\n",
    "            # 去空格\n",
    "            line = line.strip()\n",
    "            # 没有东西就退出循环，进入下一个循环\n",
    "            if not line: continue\n",
    "            \n",
    "            if not review_start and line.startswith(\"<review\"):\n",
    "                review_start = True\n",
    "                if \"label\" in line:\n",
    "                    labels.append(int(line.split('\"')[-2]))\n",
    "                continue\n",
    "            if review_start and line == \"</review>\":\n",
    "                review_start = False\n",
    "                reviews.append(\" \".join(review_text))\n",
    "                review_text = []\n",
    "                continue\n",
    "            if review_start:\n",
    "                review_text.append(line)\n",
    "        if is_pos:\n",
    "            labels = [1]*len(reviews)\n",
    "        elif not is_pos is None:\n",
    "            labels = [0]*len(reviews)\n",
    "    return reviews, labels\n",
    "    \n",
    "def process_file(path):\n",
    "    \"\"\"\n",
    "    读取训练数据和测试数据，并对它们做一些预处理\n",
    "    \"\"\"    \n",
    "    train_pos_file = os.path.join(path, \"train.positive.txt\")\n",
    "    train_neg_file = os.path.join(path, \"train.negative.txt\")\n",
    "    test_comb_file = os.path.join(path, \"test.combined.txt\")\n",
    "    \n",
    "    # TODO: 读取文件部分，把具体的内容写入到变量里面\n",
    "    train_pos_cmts, train_pos_lbs = read_data(train_pos_file, True)\n",
    "    train_neg_cmts, train_neg_lbs = read_data(train_neg_file, False)\n",
    "    train_comments = train_pos_cmts + train_neg_cmts\n",
    "    train_labels = train_pos_lbs + train_neg_lbs\n",
    "    test_comments, test_labels = read_data(test_comb_file) \n",
    "    return train_comments, train_labels, test_comments, test_labels\n",
    "\n",
    "train_comments, train_labels, test_comments, test_labels = process_file(\"./data\")\n",
    "print(train_comments[1:20])\n",
    "print(train_labels[1:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explorary Analysis: 做一些简单的可视化分析 （10分） "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8064 2500\n",
      "手感超好，而且黑色相比白色在转得时候不容易眼花，找童年的记忆啦。 1\n"
     ]
    }
   ],
   "source": [
    "# 训练数据和测试数据大小\n",
    "print (len(train_comments), len(test_comments))\n",
    "print(train_comments[1], train_labels[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: 对于训练数据中的正负样本，分别画出一个histogram， histogram的x抽是每一个样本中字符串的长度，y轴是拥有这个长度的样本的百分比。\n",
    "#       并说出样本长度是否对情感有相关性 (需要先用到结巴分词)\n",
    "#       参考：https://en.wikipedia.org/wiki/Histogram\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO： 分别列出训练数据中正负样本里的top 20单词（可以做适当的stop words removal）。 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Text Cleaning: 文本处理部分 （10分）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\lynn\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.643 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "发短信 特别 不 方便 ! 背后 的 屏幕 很大 用 不 舒服   UNK   是 手触 屏 的 ! 切换 屏幕 麻烦 ! 终于 找到 同道中人 ～ ～ ～ ～ 从 初中   UNK   就 已经 喜欢 上   UNK   但 同学 都 用 鄙夷 的 眼光 看   UNK   人为   UNK   的 样子 古怪 说 ＂ 丑 ＂ ． 气晕 ． 但 现在 有 同道中人   UNK   好开心 !   UNK   !   UNK  \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import jieba\n",
    "\n",
    "\n",
    "# TODO：对于train_comments, test_comments进行字符串的处理，几个考虑的点：\n",
    "#   1. 停用词过滤\n",
    "#   2. 去掉特殊符号\n",
    "#   3. 去掉数字（比如价格..)\n",
    "#   4. ...\n",
    "#   需要注意的点是，由于评论数据本身很短，如果去掉的太多，很可能字符串长度变成0\n",
    "#   预处理部分，可以自行选择合适的方.\n",
    "def load_stopwords(path):\n",
    "    \"\"\"\n",
    "    从外部加载停用词\n",
    "    \"\"\"\n",
    "    stopwords = set()\n",
    "    with open(path, 'r', encoding='utf-8') as in_file:\n",
    "        for line in in_file:\n",
    "            stopwords.add(line.strip())\n",
    "    return stopwords\n",
    "\n",
    "def clean_non_chinese_symbols(text):\n",
    "    \"\"\"\n",
    "    处理非中文字符\n",
    "    \"\"\"\n",
    "    text = re.sub('[!！]+', \"!\", text)\n",
    "    text = re.sub('[?？]+', \"?\", text)\n",
    "    text = re.sub(\"[a-zA-Z#$%&\\'()*+,-./:;：<=>@，。★、…【】《》“”‘’[\\\\]^_`{|}~]+\", \" UNK \", text)\n",
    "    return re.sub(\"\\s+\", \" \", text)  \n",
    "\n",
    "def clean_numbers(text):\n",
    "    \"\"\"\n",
    "    处理数字符号  128  190  NUM \n",
    "    \"\"\"\n",
    "    return re.sub(\"\\d+\", ' NUM ', text)\n",
    "\n",
    "def preprocess_text(text, stopwords):\n",
    "    \"\"\"\n",
    "    文本的预处理过程\n",
    "    \"\"\"\n",
    "    text = clean_non_chinese_symbols(text)\n",
    "    text = clean_numbers(text)\n",
    "    text = \" \".join([term for term in jieba.cut(text) if term and not term in stopwords])\n",
    "    return text\n",
    "\n",
    "path_stopwords = \"./data/stopwords-master/scu_stopwords.txt\"\n",
    "stopwords = load_stopwords(path_stopwords)\n",
    "# print(stopwords)\n",
    "train_comments_new = [preprocess_text(comment, stopwords) for comment in train_comments] \n",
    "test_comments_new = [preprocess_text(comment, stopwords) for comment in test_comments]\n",
    "print(train_comments_new[0], test_comments_new[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction : 从文本中提取特征 （10分）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8064, 22858) (2500, 22858) (8064,) (2500,)\n"
     ]
    }
   ],
   "source": [
    "# TODO: 利用tf-idf从文本中提取特征,写到数组里面. \n",
    "#       参考：https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer()\n",
    "X_train = tfidf.fit_transform(train_comments_new) # 训练数据的特征\n",
    "y_train = train_labels # 训练数据的label\n",
    "X_test = tfidf.transform(test_comments_new)  # 测试数据的特征\n",
    "y_test = test_labels  # 测试数据的label\n",
    "\n",
    "print (np.shape(X_train), np.shape(X_test), np.shape(y_train), np.shape(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling: 训练模型以及选择合适的超参数 （25分）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Tuning hyper-parameters for precision\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'C': 1}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "0.714130  with:   {'C': 0.1}\n",
      "0.753253  with:   {'C': 1}\n",
      "0.737433  with:   {'C': 10}\n",
      "0.711563  with:   {'C': 100}\n",
      "0.688785  with:   {'C': 1000}\n",
      "\n",
      "Detailed classification report:\n",
      "\n",
      "The model is trained on the full development set.\n",
      "The scores are computed on the full evaluation set.\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.49      0.62      1250\n",
      "           1       0.64      0.91      0.75      1250\n",
      "\n",
      "    accuracy                           0.70      2500\n",
      "   macro avg       0.74      0.70      0.69      2500\n",
      "weighted avg       0.74      0.70      0.69      2500\n",
      "\n",
      "\n",
      "# Tuning hyper-parameters for recall\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'C': 1}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "0.651909  with:   {'C': 0.1}\n",
      "0.747519  with:   {'C': 1}\n",
      "0.741568  with:   {'C': 10}\n",
      "0.717013  with:   {'C': 100}\n",
      "0.693575  with:   {'C': 1000}\n",
      "\n",
      "Detailed classification report:\n",
      "\n",
      "The model is trained on the full development set.\n",
      "The scores are computed on the full evaluation set.\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.49      0.62      1250\n",
      "           1       0.64      0.91      0.75      1250\n",
      "\n",
      "    accuracy                           0.70      2500\n",
      "   macro avg       0.74      0.70      0.69      2500\n",
      "weighted avg       0.74      0.70      0.69      2500\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.metrics import classification_report\n",
    "# TODO： 利用逻辑回归来训练模型\n",
    "#       1. 评估方式： F1-score\n",
    "#       2. 超参数（hyperparater）的选择利用grid search https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html\n",
    "#       3. 打印出在测试数据中的最好的结果（precision, recall, f1-score, 需要分别打印出正负样本，以及综合的）\n",
    "#       请注意：做交叉验证时绝对不能用测试数据。 测试数据只能用来最后的”一次性“检验。\n",
    "#       逻辑回归的使用方法请参考：http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "#       对于逻辑回归，经常调整的超参数为： C\n",
    "\n",
    "# 设置gridsearch的参数\n",
    "tuned_parameters = [{'C': [0.1, 1, 10, 100, 1000]}]\n",
    " \n",
    "#设置模型评估的方法.如果不清楚,可以参考上面的k-fold章节里面的超链接\n",
    "scores = ['precision', 'recall']\n",
    " \n",
    "for score in scores:\n",
    "    print(\"# Tuning hyper-parameters for %s\" % score)\n",
    "    print()\n",
    " \n",
    "    #构造这个GridSearch的分类器,5-fold\n",
    "    clf = GridSearchCV(LogisticRegression(solver='liblinear'), tuned_parameters, cv=5,\n",
    "                       scoring='%s_weighted' % score)\n",
    "    #只在训练集上面做k-fold,然后返回最优的模型参数\n",
    "    clf.fit(X_train, y_train)\n",
    " \n",
    "    print(\"Best parameters set found on development set:\")\n",
    "    print()\n",
    "    #输出最优的模型参数\n",
    "    print(clf.best_params_)\n",
    "    print()\n",
    "    print(\"Grid scores on development set:\")\n",
    "    print()\n",
    "  \n",
    "    means = clf.cv_results_['mean_test_score']\n",
    "    params = clf.cv_results_['params']\n",
    "    for mean,param in zip(means,params):\n",
    "        print(\"%f  with:   %r\" % (mean,param))\n",
    "\n",
    "    print()\n",
    " \n",
    "    print(\"Detailed classification report:\")\n",
    "    print()\n",
    "    print(\"The model is trained on the full development set.\")\n",
    "    print(\"The scores are computed on the full evaluation set.\")\n",
    "    print()\n",
    "    #在测试集上测试最优的模型的泛化能力.\n",
    "    y_true, y_pred =y_test, clf.predict(X_test)\n",
    "    # classification_report函数用于显示主要分类指标的文本报告．在报告中显示每个类的精确度，召回率，F1值等信息\n",
    "    print(classification_report(y_true, y_pred))  \n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Tuning hyper-parameters for precision\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'C': 1, 'gamma': 0.001, 'kernel': 'linear'}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "0.746843  with:   {'C': 1, 'gamma': 0.001, 'kernel': 'linear'}\n",
      "0.746843  with:   {'C': 1, 'gamma': 0.0001, 'kernel': 'linear'}\n",
      "0.704763  with:   {'C': 10, 'gamma': 0.001, 'kernel': 'linear'}\n",
      "0.704763  with:   {'C': 10, 'gamma': 0.0001, 'kernel': 'linear'}\n",
      "0.671803  with:   {'C': 100, 'gamma': 0.001, 'kernel': 'linear'}\n",
      "0.671803  with:   {'C': 100, 'gamma': 0.0001, 'kernel': 'linear'}\n",
      "0.659212  with:   {'C': 1000, 'gamma': 0.001, 'kernel': 'linear'}\n",
      "0.659212  with:   {'C': 1000, 'gamma': 0.0001, 'kernel': 'linear'}\n",
      "\n",
      "Detailed classification report:\n",
      "\n",
      "The model is trained on the full development set.\n",
      "The scores are computed on the full evaluation set.\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.54      0.65      1250\n",
      "           1       0.66      0.89      0.76      1250\n",
      "\n",
      "    accuracy                           0.72      2500\n",
      "   macro avg       0.75      0.72      0.71      2500\n",
      "weighted avg       0.75      0.72      0.71      2500\n",
      "\n",
      "\n",
      "# Tuning hyper-parameters for recall\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'C': 1, 'gamma': 0.001, 'kernel': 'linear'}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "0.748759  with:   {'C': 1, 'gamma': 0.001, 'kernel': 'linear'}\n",
      "0.748759  with:   {'C': 1, 'gamma': 0.0001, 'kernel': 'linear'}\n",
      "0.710193  with:   {'C': 10, 'gamma': 0.001, 'kernel': 'linear'}\n",
      "0.710193  with:   {'C': 10, 'gamma': 0.0001, 'kernel': 'linear'}\n",
      "0.678695  with:   {'C': 100, 'gamma': 0.001, 'kernel': 'linear'}\n",
      "0.678695  with:   {'C': 100, 'gamma': 0.0001, 'kernel': 'linear'}\n",
      "0.665549  with:   {'C': 1000, 'gamma': 0.001, 'kernel': 'linear'}\n",
      "0.665549  with:   {'C': 1000, 'gamma': 0.0001, 'kernel': 'linear'}\n",
      "\n",
      "Detailed classification report:\n",
      "\n",
      "The model is trained on the full development set.\n",
      "The scores are computed on the full evaluation set.\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.54      0.65      1250\n",
      "           1       0.66      0.89      0.76      1250\n",
      "\n",
      "    accuracy                           0.72      2500\n",
      "   macro avg       0.75      0.72      0.71      2500\n",
      "weighted avg       0.75      0.72      0.71      2500\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "# TODO： 利用SVM来训练模型\n",
    "#       1. 评估方式： F1-score\n",
    "#       2. 超参数（hyperparater）的选择利用grid search https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html\n",
    "#       3. 打印出在测试数据中的最好的结果（precision, recall, f1-score, 需要分别打印出正负样本，以及综合的）\n",
    "#       请注意：做交叉验证时绝对不能用测试数据。 测试数据只能用来最后的”一次性“检验。\n",
    "#       SVM的使用方法请参考：http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\n",
    "#       对于SVM模型，经常调整的超参数为：C, gamma, kernel\n",
    "\n",
    "\n",
    "# 设置gridsearch的参数\n",
    "tuned_parameters = [{'kernel': ['linear'], 'gamma': [1e-3, 1e-4],\n",
    "                     'C': [1, 10, 100, 1000]}]\n",
    " \n",
    "#设置模型评估的方法.如果不清楚,可以参考上面的k-fold章节里面的超链接\n",
    "scores = ['precision', 'recall']\n",
    " \n",
    "for score in scores:\n",
    "    print(\"# Tuning hyper-parameters for %s\" % score)\n",
    "    print()\n",
    " \n",
    "    #构造这个GridSearch的分类器,5-fold\n",
    "    clf = GridSearchCV(svm.SVC(), tuned_parameters, cv=5,\n",
    "                       scoring='%s_weighted' % score)\n",
    "    #只在训练集上面做k-fold,然后返回最优的模型参数\n",
    "    clf.fit(X_train, y_train)\n",
    " \n",
    "    print(\"Best parameters set found on development set:\")\n",
    "    print()\n",
    "    #输出最优的模型参数\n",
    "    print(clf.best_params_)\n",
    "    print()\n",
    "    print(\"Grid scores on development set:\")\n",
    "    print()\n",
    "  \n",
    "    means = clf.cv_results_['mean_test_score']\n",
    "    params = clf.cv_results_['params']\n",
    "    for mean,param in zip(means,params):\n",
    "        print(\"%f  with:   %r\" % (mean,param))\n",
    "\n",
    "    print()\n",
    " \n",
    "    print(\"Detailed classification report:\")\n",
    "    print()\n",
    "    print(\"The model is trained on the full development set.\")\n",
    "    print(\"The scores are computed on the full evaluation set.\")\n",
    "    print()\n",
    "    #在测试集上测试最优的模型的泛化能力.\n",
    "    y_true, y_pred = y_test, clf.predict(X_test)\n",
    "    # classification_report函数用于显示主要分类指标的文本报告．在报告中显示每个类的精确度，召回率，F1值等信息\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于超参数的调整，我们经常使用gridsearch，这也是工业界最常用的方法，但它的缺点是需要大量的计算，所以近年来这方面的研究也成为了重点。 其中一个比较经典的成果为Bayesian Optimization（利用贝叶斯的思路去寻找最好的超参数）。Ryan P. Adams主导的Bayesian Optimization利用高斯过程作为后验概率（posteior distribution）来寻找最优解。 https://papers.nips.cc/paper/4522-practical-bayesian-optimization-of-machine-learning-algorithms.pdf 在下面的练习中，我们尝试使用Bayesian Optimization工具来去寻找最优的超参数。参考工具：https://github.com/fmfn/BayesianOptimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   |     C     |   gamma   |\n",
      "-------------------------------------------------\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'SVC' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32md:\\ProgramData\\Anaconda3\\envs\\ten-gpu1.14\\lib\\site-packages\\bayes_opt\\target_space.py\u001b[0m in \u001b[0;36mprobe\u001b[1;34m(self, params)\u001b[0m\n\u001b[0;32m    190\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 191\u001b[1;33m             \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cache\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0m_hashable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    192\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: (919.7123709051251, 0.0008002012226013459)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-f61aab927e7e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     21\u001b[0m optimizer.maximize(\n\u001b[0;32m     22\u001b[0m     \u001b[0minit_points\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1e-4\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m     \u001b[0mn_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1e-4\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m )\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\ProgramData\\Anaconda3\\envs\\ten-gpu1.14\\lib\\site-packages\\bayes_opt\\bayesian_optimization.py\u001b[0m in \u001b[0;36mmaximize\u001b[1;34m(self, init_points, n_iter, acq, kappa, xi, **gp_params)\u001b[0m\n\u001b[0;32m    172\u001b[0m                 \u001b[0miteration\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    173\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 174\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprobe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_probe\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlazy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    175\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    176\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOPTIMIZATION_END\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\ProgramData\\Anaconda3\\envs\\ten-gpu1.14\\lib\\site-packages\\bayes_opt\\bayesian_optimization.py\u001b[0m in \u001b[0;36mprobe\u001b[1;34m(self, params, lazy)\u001b[0m\n\u001b[0;32m    110\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_queue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 112\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_space\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprobe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    113\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOPTIMIZATION_STEP\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\ProgramData\\Anaconda3\\envs\\ten-gpu1.14\\lib\\site-packages\\bayes_opt\\target_space.py\u001b[0m in \u001b[0;36mprobe\u001b[1;34m(self, params)\u001b[0m\n\u001b[0;32m    192\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    193\u001b[0m             \u001b[0mparams\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_keys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 194\u001b[1;33m             \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    195\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    196\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'SVC' object is not callable"
     ]
    }
   ],
   "source": [
    "# TODO: 仍然使用SVM模型，但在这里使用Bayesian Optimization来寻找最好的超参数。 \n",
    "#       1. 评估方式： F1-score\n",
    "#       2. 超参数（hyperparater）的选择利用Bayesian Optimization https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html\n",
    "#       3. 打印出在测试数据中的最好的结果（precision, recall, f1-score, 需要分别打印出正负样本，以及综合的）\n",
    "#       请注意：做交叉验证时绝对不能用测试数据。 测试数据只能用来最后的”一次性“检验。\n",
    "#       SVM的使用方法请参考：http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\n",
    "#       对于SVM模型，经常调整的超参数为：C, gamma, kernel\n",
    "#       参考Bayesian Optimization开源工具： https://github.com/fmfn/BayesianOptimization\n",
    "from bayes_opt import BayesianOptimization\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "pbounds = {'gamma': (1e-4, 1e-3), 'C': (1,1000)}\n",
    "\n",
    "\n",
    "#设置模型评估的方法.如果不清楚,可以参考上面的k-fold章节里面的超链接\n",
    "optimizer = BayesianOptimization(\n",
    "    f=svm.SVC(kernel='linear'),\n",
    "    pbounds=pbounds\n",
    ")\n",
    "optimizer.maximize(\n",
    "    init_points=2,\n",
    "    n_iter=3,\n",
    ")\n",
    "\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "#在测试集上测试最优的模型的泛化能力.\n",
    "y_true, y_pred = y_test, clf.predict(X_test)\n",
    "\n",
    "# classification_report函数用于显示主要分类指标的文本报告．在报告中显示每个类的精确度，召回率，F1值等信息\n",
    "print(classification_report(y_true, y_pred))\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 特征: 添加n-gram特征 (10分)\n",
    "在原有tf-idf特征的基础上，添加n-gram特征（在这里我们使用bi-gram特征）。添加完之后效果是否有提升？ 为什么？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train =  # 添加完bigram之后的特征\n",
    "y_train =  # \n",
    "X_test =   # 添加完bigram之后的特征\n",
    "y_test =   # \n",
    "\n",
    "print (np.shape(X_train), np.shape(X_test), np.shape(y_train), np.shape(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO 模型的训练，如上"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extension （10分）\n",
    "1. 对于情感分析来说，有一个问题也很重要，比如一个句子里出现了 “我不太兴奋”， “不是很满意”。 在这种情况，因为句子中出现了一些积极的词汇很可能被算法识别成正面的，但由于前面有一个“不”这种关键词，所以否定+肯定=否定，算法中这种情况也需要考虑。另外，否定+否定=肯定， 这种情况也一样。 \n",
    "2. 另外一个问题是aspect-based sentiment analysis, 这个指的是做情感分析的时候，我们既想了解情感，也想了解特定的方面。 举个例子： “这部手机的电池性能不错，但摄像不够清晰啊!”, 分析完之后可以得到的结论是： “电池：正面， 摄像：负面”， 也就是针对于一个产品的每一个性能做判定，这种问题我们叫做aspect-based sentiment analysis，也是传统情感分析的延伸。\n",
    "\n",
    "Q: 对于如上两个问题，有什么解决方案？ 大概列一下能想到的处理方案。 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 拓展建议\n",
    "如果想挑战一下自己，把上述的问题也实现一下，但不计为本项目的分数。建议好好整理一下代码，上传到github。 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 其他领域\n",
    "跟情感分析类似的领域有叫affective computing, 也就是用来识别情绪(emotion recognition)。但情感和情绪又不太一样，情绪指的是高兴，低落，失落，兴奋这些人的情绪。我们知道真正的人工智能是需要读懂人类的情绪的。而且情绪识别有很多场景，比如服务机器人根据不同的情绪来跟用户交流； 无人驾驶里通过识别用户的情绪（摄像头或者声音或者传感器）来保证安全驾驶； IOT领域里设备也需要读懂我们的情绪； 微博里通过文本读懂每个人发文时的情绪。 \n",
    "\n",
    "总体来讲，情绪识别跟情感识别所用到的技术是类似的，感兴趣的小伙伴，也可以关注一下这个领域。 如果想发论文，强烈建议选择情绪方面的，不建议选择情感分析，因为问题太老了。情绪分析是近几年才开始受关注的领域。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ten-gpu1.14",
   "language": "python",
   "name": "ten-gpu1.14"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
